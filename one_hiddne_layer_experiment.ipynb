{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "one_hiddne_layer_experiment.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyP7auLTLFo3UgZTDdoxyWsi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tayfununal/2-Hidden-Layer-Neural-Networks/blob/master/one_hiddne_layer_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhiLHNFTPX0Q"
      },
      "source": [
        "!pip install playground-data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQE3w5lwfM6j"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import plygdata as pg\n",
        "import json\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwvBuH2-_Zkh"
      },
      "source": [
        "Datas = json.loads(requests.get(\"https://raw.githubusercontent.com/tayfununal/2-Hidden-Layer-Neural-Networks/master/produced_XOR_Datas.json\").text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbfvdBk-Iw09"
      },
      "source": [
        "data_one = np.array(Datas['1'])\n",
        "X_train, y_train, X_test, y_test = pg.split_data(data_one, validation_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c37fky0Pz69"
      },
      "source": [
        "X_train = X_train.T\n",
        "y_train = y_train.T\n",
        "X_test = X_test.T\n",
        "y_test = y_test.T\n",
        "print(\"Shape of X_train:\", X_train.shape,\n",
        "      \"\\nShape of y_train:\", y_train.shape,\n",
        "      \"\\nShape of X_test:\", X_test.shape,\n",
        "      \"\\nShape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liPL5lGCQGXD"
      },
      "source": [
        "def initialization_parameters(x, y, num_node):\n",
        "  W1 = np.random.randn(num_node * x.shape[0]).reshape(num_node, x.shape[0]) * 0.001\n",
        "  b1 = np.zeros((num_node,1))\n",
        "\n",
        "  W2 = np.random.randn(y.shape[0],num_node) * 0.001\n",
        "  b2 = np.zeros((y.shape[0],1))\n",
        "\n",
        "  assert W1.shape == (num_node,x.shape[0])\n",
        "  assert b1.shape == (num_node, 1)\n",
        "\n",
        "  assert W2.shape == (y.shape[0], num_node)\n",
        "  assert b2.shape == (y.shape[0], 1)\n",
        "\n",
        "  parameters = {'W1':W1,\n",
        "                'b1':b1,\n",
        "                'W2':W2,\n",
        "                'b2':b2}\n",
        "  return parameters\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "def reluDerivative(x):\n",
        "  x[x<=0] = 0\n",
        "  x[x>0] = 1\n",
        "  return x\n",
        "\n",
        "def forward_prop(x,parameters):\n",
        "  W1 = parameters['W1']\n",
        "  b1 = parameters['b1']\n",
        "  W2 = parameters['W2']\n",
        "  b2 = parameters['b2']\n",
        "  \n",
        "  Z1 = np.dot(W1, x) + b1\n",
        "  A1 = relu(Z1)\n",
        "  Z2 = np.dot(W2, A1) + b2\n",
        "  A2 = sigmoid(Z2)\n",
        "\n",
        "  assert (A2.shape == (1, x.shape[1]))\n",
        "  cache = {\n",
        "      'Z1' : Z1,\n",
        "      'A1' : A1,\n",
        "      'Z2' : Z2,\n",
        "      'A2' : A2\n",
        "  }\n",
        "  return A2, cache\n",
        "\n",
        "def backward_prop(x, y, parameters, cache, learning_rate = 0.1, lambd=0.1):\n",
        "  m = y.shape[1]\n",
        "  \n",
        "  W1 = parameters['W1']\n",
        "  b1 = parameters['b1']\n",
        "  W2 = parameters['W2']\n",
        "  b2 = parameters['b2']\n",
        "\n",
        "  A1 = cache['A1']\n",
        "  A2 = cache['A2']\n",
        "  \n",
        "  # Calculations of backward propagation: dW1, db1, dW2, db2\n",
        "  dZ2 = A2 - y\n",
        "  dW2 = (1 / m) * np.dot(dZ2, A1.T) + (lambd / m) * W2\n",
        "  db2 = (1 / m) * np.sum(dZ2, axis = 1, keepdims = True)\n",
        "\n",
        "  dZ1 = np.multiply(np.dot(W2.T, dZ2), reluDerivative(A1))\n",
        "  dW1 = (1 / m) * np.dot(dZ1, x.T) + (lambd / m) * W1  \n",
        "  db1 = (1 / m) * np.sum(dZ1 , axis = 1, keepdims = True)\n",
        "\n",
        "  # Updating parameters\n",
        "  W1 = W1 - learning_rate * dW1\n",
        "  b1 = b1 - learning_rate * db1\n",
        "  W2 = W2 - learning_rate * dW2\n",
        "  b2 = b2 - learning_rate * db2\n",
        "  \n",
        "  parameters = {'W1':W1,\n",
        "                'b1':b1,\n",
        "                'W2':W2,\n",
        "                'b2':b2}\n",
        "  return parameters\n",
        "\n",
        "def cross_entropy_cost(y, A2, parameters, lambd = 0.1):\n",
        "  m = y.shape[1]\n",
        "\n",
        "  W1 = parameters[\"W1\"]\n",
        "  W2 = parameters[\"W2\"]\n",
        "  \n",
        "  cross_entropy = np.multiply(np.log(A2 + 1e-15), y) + np.multiply((1 - y), np.log(1 - A2 + 1e-15))\n",
        "  L2_regularization_cost = (np.sum(np.square(W1)) + np.sum(np.square(W2))) * (lambd / (2 * m))\n",
        "\n",
        "  cost = (- 1.0 / m) * np.sum(cross_entropy) + L2_regularization_cost\n",
        "\n",
        "  # Squeezing to avoid unnecessary dimensions \n",
        "  cost = np.squeeze(cost) \n",
        "  return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq0KrPZ_Whua"
      },
      "source": [
        "def nn_model(x, y, parameters, number_of_iter = 1000):\n",
        "  cost_value = {}\n",
        "  A2, cache = forward_prop(x, parameters)\n",
        "  cost = cross_entropy_cost(y, A2 ,parameters)\n",
        "  cost_value[1] = cost\n",
        "\n",
        "  for i in range(1,number_of_iter):\n",
        "    parameters = backward_prop(x, y, parameters, cache, learning_rate=0.3, lambd=0.3)\n",
        "    A2, cache = forward_prop(x, parameters)\n",
        "    cost = cross_entropy_cost(y, A2, parameters)\n",
        "    \n",
        "    #cost_value[i+1] = cost\n",
        "    if (i+1) % 100 == 0:\n",
        "     cost_value[i+1] = cost\n",
        "     print(\"%i.\"%(i+1),cost)\n",
        "     \n",
        "  return cost_value, parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWNKWdirWhtc"
      },
      "source": [
        "np.random.seed(656562)\n",
        "parameters = initialization_parameters(X_test, y_test, 10)\n",
        "cost_value, parameters = nn_model(X_train, y_train, parameters, number_of_iter=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a51RQu_KXO4r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}